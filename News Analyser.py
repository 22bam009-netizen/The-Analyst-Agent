# -*- coding: utf-8 -*-
"""Copy of Untitled39.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N6X-0NTMbErTwGVZUSn7WwiXZzqaOwuA
"""

!pip install feedparser sentence-transformers transformers pandas

import feedparser
import pandas as pd
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
import json
import os

embedder = SentenceTransformer("all-MiniLM-L6-v2")
extractor = pipeline("text2text-generation", model="google/flan-t5-base")


RSS_FEEDS = [
    "https://techcrunch.com/tag/artificial-intelligence/feed/",
    "https://venturebeat.com/category/ai/feed/",
    "https://www.theverge.com/rss/artificial-intelligence/index.xml",
    "https://research.google/blog/rss/",
    "https://news.mit.edu/rss/topic/artificial-intelligence2",
    "https://www.artificial-intelligence.blog/ai-news?format=rss"
]


def fetch_rss_news():
    articles = []
    for url in RSS_FEEDS:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            articles.append({
                "title": entry.title,
                "description": entry.get("summary", ""),
                "content": entry.get("summary", ""),
                "published": entry.get("published", ""),
                "source": url
            })
    return articles


print("Fetching RSS articles...")
articles = fetch_rss_news()
print("Fetched:", len(articles))


def compute_information_density(article):
    # Truncate text to fit model's max input length (e.g., 400 characters for a 512 token limit)
    text = (article.get("content") or article.get("description") or "").lower()
    text = text[:400] # Truncate to avoid model input length issues

    tech_keywords = [
        "model", "dataset", "training", "api", "benchmark",
        "neural", "architecture", "open-source", "pipeline",
        "compute", "latency", "accuracy", "deployment",
        "funding", "investment", "acquisition", "seed", "series"
    ]
    fluff_words = [
        "innovative", "revolutionary", "leading", "cutting-edge",
        "empowers", "game-changing", "seamless", "unprecedented",
        "visionary", "transformative"
    ]

    tech_hits = sum(k in text for k in tech_keywords)
    fluff_hits = sum(k in text for k in fluff_words)

    length_score = min(len(text) / 500, 1.0)

    prompt = f"""
Rate how technical and information-dense this article is from 0 to 1.

Article:
{text}

Respond only with a number between 0 and 1.
    """

    try:
        depth = extractor(prompt, max_new_tokens=10, do_sample=False)[0]["generated_text"]
        depth = float(depth.strip())
        depth = max(0, min(depth, 1))
    except:
        depth = 0.3

    info_density = (
        0.40 * (tech_hits / 8) +
        0.30 * length_score +
        0.30 * depth -
        0.20 * (fluff_hits / 10)
    )

    return info_density


def filter_low_density_articles(articles, threshold=0.35):
    result = []
    for art in articles:
        score = compute_information_density(art)
        art["info_density"] = round(score, 3)
        if score >= threshold:
            result.append(art)
    return result


print("Filtering low-density articles...")
articles = filter_low_density_articles(articles)
print("After info-density filter:", len(articles))


def extract_fields(article):
    # Truncate text to fit model's max input length (e.g., 400 characters for a 512 token limit)
    text = article["title"] + "\n" + article["content"]
    text = text[:400] # Truncate to avoid model input length issues

    prompt = f"""
Extract the following fields from the news article:

Return JSON only with:
company_name
category
sentiment_score (0 to 1)
is_funding_news (true/false)

Article:
{text}
"""

    try:
        output = extractor(prompt, max_new_tokens=200, do_sample=False)[0]["generated_text"]
        data = json.loads(output)
    except:

        data = {
            "company_name": "",
            "category": "",
            "sentiment_score": 0.5,
            "is_funding_news": False
        }


    data["title"] = article["title"]
    data["published"] = article["published"]
    data["source"] = article["source"]
    data["info_density"] = article["info_density"]

    return data


print("Extracting structured JSON...")
structured = [extract_fields(a) for a in articles]
print("Extracted:", len(structured))


def deduplicate_articles(data, title_threshold=0.78, summary_threshold=0.70):
    unique = []
    stored_titles = []
    stored_summaries = []

    for art in data:
        title_emb = embedder.encode(art["title"], convert_to_tensor=True)
        summary_emb = embedder.encode(art["category"], convert_to_tensor=True)

        duplicate = False

        for i in range(len(stored_titles)):
            sim_title = float(util.pytorch_cos_sim(title_emb, stored_titles[i]))
            sim_summary = float(util.pytorch_cos_sim(summary_emb, stored_summaries[i]))
            combined = (0.6 * sim_title) + (0.4 * sim_summary)

            if sim_title > title_threshold or sim_summary > summary_threshold or combined > 0.75:
                duplicate = True
                break

        if not duplicate:
            unique.append(art)
            stored_titles.append(title_emb)
            stored_summaries.append(summary_emb)

    return unique


print("Deduplicating...")
clean = deduplicate_articles(structured)
print("Final cleaned articles:", len(clean))


csv_path = "AI_NEWS.csv"

df_new = pd.DataFrame(clean)

if os.path.exists(csv_path):
    df_old = pd.read_csv(csv_path)
    df_final = pd.concat([df_old, df_new], ignore_index=True)
else:
    df_final = df_new

df_final.to_csv(csv_path, index=False)

print("Saved to:", csv_path)